---
title: "Time-VLM: Exploring Multimodal Vision-Language Models for Augmented Time Series Forecasting"
date: 2025-05-01pub_last: ' <span class="badge badge-pill badge-publication badge-warning">Poster</span>'
 00:00:00 +0800
selected: true
pub:            "International Conference on Machine Learning"
# pub_pre:        "Submitted to "
# pub_post: "Under review."
pub_last: ' <span class="badge badge-pill badge-publication badge-warning">Poster</span>'
pub_date: "2025"

abstract: >-
  Propose Time-VLM, a novel multimodal framework that leverages pre-trained Vision-Language Models (VLMs) to bridge temporal, visual, and textual modalities for enhanced time series forecasting.
cover: /assets/images/covers/TimeVLM.png
authors:
  - Siru Zhong
  - Weilin Ruan
  - Min Jin
  - Huan Li
  - Qingsong Wen
  - Yuxuan Liang
links:
  Paper: https://arxiv.org/abs/2502.04395
---
